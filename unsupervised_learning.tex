\subsection{Unsupervised Learning}
While supervised learning aims to entangle patterns by examining data that has been labeled by humans, unsupervised learning attempts to do the same with unlabeled data. This requires algorithms that can examine unstructured information and find commonalities and dissimilarities between data points. Humans are exceptional at unsupervised learning, performing tasks of separation and discrimination from a young age. For example, children "can discriminate faces and voices by sex, habituate to faces of both sexes, and make inter-modal associations between faces and voices" \cite{martin2010patterns}. These skills to separate items based on patterns are also evident in our everyday lives: we separate tasks into work, pleasure, or home-related categories; cars into new, old, attractive, or boring; restaurants into lower-end (e.g. McDonalds) or high-end (e.g. Red Lobster); people into races. In most if not all of the previous cases, we are not \textit{told} what items belong to what categories, and in fact we are not told what the categories are. Rather, we assume that there are some categories, and we attempt to distribute items into the categories that make the most sense to us. This human ability to discern patterns from unstructured data is similar to how unsupervised learning works.

With most unsupervised learning algorithms, the number of categories is known or guessed, and it is the job of the algorithm to separate the data into that many categories in the best possible way. A plethora of unsupervised learning algorithms exist, most of which are extensions on more basic and fundamental ones. Some of the fundamental algorithms include K-means clustering, fuzzy clustering, single linkage clustering, self organized maps, and vector quantization. For the purposes of this work, we only cover the simple K-means algorithm, an extension of which we use in the experimental section of this paper.

\subsubsection{K-means Clustering}
K-means clustering is a powerful tool used in various fields mainly for cluster analysis. It operates on basic principles of finding subsets of data that share the most commonality between one another. This algorithm requires the data to be in continuous numerical form, and uses one of several measures of distance to attribute closeness. The most widely used measure of distance is the Euclidean distance 
$$
D(x, y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_n - y_n)^2}.
$$
K-means works by finding $k$ centers of clusters, where $k$ is the number of hypothesized clusters given by the user. This goal is achieved by first choosing $k$ random points from all the points in the given dataset. All the remaining points are then matched with the closest center, and each matched set of points is declared as a cluster. For centers $c_1, c_2, \dots, c_n$ and point $x$, $x$ belongs to the cluster whose center is the closest to $x$ 
$$
Cluster(x) = \arg \min_{c} \Big( D(c_1, x), D(c_2, x), \dots, D(c_k, x) \Big).
$$
Once all the points have been assigned to a cluster, an approximate center point of each cluster is calculated as the mean of all the points in that cluster
$$
\hat c_i = mean(x_{i,1}, x_{i,2}, \dots, x_{i,l})
$$
where $x_{i,j}$ is the $j^{th}$ point in the $i^{th}$ cluster.
A new center for the cluster, $c_i$, is then chosen, with the new center being the closest to the approximate center point
$$
c_i = \arg \min_{x} \Big( D(\hat c_i, x_{i, 1}), D(\hat c_i, x_{i, 2}), \dots, D(\hat c_i, x_{i, 2}) \Big ) .
$$
This will result in a change of points-to-cluster-center distances, and will therefore change the clusters that some points belong to, which will change the cluster centers, resulting in a cycle of improvement that has been proven to converge with the repetition of the center assignment steps (Fig. \ref{fig:k-means}). While the K-means algorithm is useful in its own right, extensions have made it more powerful. An example of such an extension is the Gaussian Mixture model, which is a more generalized form of the K-means that assumes there are $k$ uniform distributions with unknown parameters. This generalization allows the model to group clusters using several covariance shapes, meaning the distance relationships are much more diverse and flexible (Fig. \ref{fig:gaussian_mixture}) .

\begin{figure}[!h]
  \centering
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/k-means_first_1_iteration.pdf}
    \caption{Iteration 1}
  \end{subfigure} %
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/k-means_first_2_iteration.pdf}
    \caption{Iteration 2}
  \end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/k-means_first_5_iteration.pdf}
		\caption{Iteration 5}
	\end{subfigure}
  \caption{A sample iteration of simple k-means clustering on the generally available Iris dataset. The clusters are shown in different colors, while the centers are the green circles. In the first iteration, the centers are randomized along the dataset. As the algorithm iterates, it finds more representatives cluster centers which split the dataset more evenly, until it converges in step 5.}
  \label{fig:k-means}
\end{figure}

\begin{figure}[!h]
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/simple_k_means_iris.pdf}
    \caption{Simple k-means}
  \end{subfigure} %
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/gaussian_mixture_iris.pdf}
    \caption{Gaussian Mixture}
  \end{subfigure}
  \caption{While the standard k-means algorithm splits the dataset in the most even way, that does not necessarily result in a meaningful or accurate separation of clusters. The Gaussian Mixture algorithm takes advantage of more assumptions such as assuming that the clusters are produced from $k$ different normal (or Gaussian) distributions, and attributes points to clusters that make these distributions the most likely. This usually results in a more meaningful approach and result, as many natural phenomena are indeed a product of the Gaussian distribution.}
  \label{fig:gaussian_mixture}
\end{figure}